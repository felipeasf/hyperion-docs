{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Hyperion History API Scalable Full History API Solution for EOSIO based blockchains Made with \u2665 by EOS Rio Introducing an storage-optimized action format for EOSIO The original history_plugin bundled with eosio, that provided the v1 api, stored inline action traces nested inside their root actions. This led to an excessive amount of data being stored and also transferred whenever a user requested the action history for a given account. Also inline actions are used as a \"event\" mechanism to notify parties on a transaction. Based on those Hyperion implements some changes actions are stored in a flattened format a parent field is added to the inline actions to point to the parent global sequence if the inline action data is identical to the parent it is considered a notification and thus removed from the database no blocks or transaction data is stored, all information can be reconstructed from actions With those changes the API format focus on delivering faster search times, lower bandwidth overhead and easier usability for UI/UX developers.","title":"Home"},{"location":"#hyperion-history-api","text":"Scalable Full History API Solution for EOSIO based blockchains Made with \u2665 by EOS Rio","title":"Hyperion History API"},{"location":"#introducing-an-storage-optimized-action-format-for-eosio","text":"The original history_plugin bundled with eosio, that provided the v1 api, stored inline action traces nested inside their root actions. This led to an excessive amount of data being stored and also transferred whenever a user requested the action history for a given account. Also inline actions are used as a \"event\" mechanism to notify parties on a transaction. Based on those Hyperion implements some changes actions are stored in a flattened format a parent field is added to the inline actions to point to the parent global sequence if the inline action data is identical to the parent it is considered a notification and thus removed from the database no blocks or transaction data is stored, all information can be reconstructed from actions With those changes the API format focus on delivering faster search times, lower bandwidth overhead and easier usability for UI/UX developers.","title":"Introducing an storage-optimized action format for EOSIO"},{"location":"data/","text":"Action Data Structure (work in progress) @timestamp - block time global_sequence - unique action global_sequence, used as index id parent - points to the parent action (in the case of an inline action) or equal to 0 if root level block_num - block number where the action was processed trx_id - transaction id producer - block producer act account - contract account name - contract method name authorization - array of signers actor - signing actor permission - signing permission data - action data input object account_ram_deltas - array of ram deltas and payers account delta notified - array of accounts that were notified (via inline action events)","title":"Data Structure"},{"location":"data/#action-data-structure-work-in-progress","text":"@timestamp - block time global_sequence - unique action global_sequence, used as index id parent - points to the parent action (in the case of an inline action) or equal to 0 if root level block_num - block number where the action was processed trx_id - transaction id producer - block producer act account - contract account name - contract method name authorization - array of signers actor - signing actor permission - signing permission data - action data input object account_ram_deltas - array of ram deltas and payers account delta notified - array of accounts that were notified (via inline action events)","title":"Action Data Structure (work in progress)"},{"location":"docker/","text":"Soon","title":"Docker"},{"location":"install/","text":"Getting Started Installation Dependencies This setup has only been tested with Ubuntu 18.04, but should work with other OS versions too Elasticsearch 7.4.X RabbitMQ Redis Node.js v12 PM2 Nodeos 1.8.4 w/ state_history_plugin and chain_api_plugin Note The indexer requires redis, pm2 and node.js to be on the same machine. Other dependencies might be installed on other machines, preferably over a very high speed and low latency network. Indexing speed will vary greatly depending on this configuration. Elasticsearch Installation Info Follow the detailed installation instructions on the official elasticsearch documentation 1. Edit /etc/elasticsearch/elasticsearch.yml cluster.name: myCluster bootstrap.memory_lock: true 2. Edit /etc/elasticsearch/jvm.options # Set your heap size, avoid allocating more than 31GB, even if you have enought RAM. # Test on your specific machine by changing -Xmx32g in the following command: # java -Xmx32g -XX:+UseCompressedOops -XX:+PrintFlagsFinal Oops | grep Oops -Xms16g -Xmx16g 3. Disable swap run swapoff -a this will immediately disable swap remove any swap entry from /etc/fstab Tip Identify configured swap devices and files with cat /proc/swaps 4. Allow memlock run sudo systemctl edit elasticsearch and add the following lines [Service] LimitMEMLOCK=infinity 5. Start elasticsearch and check the logs (verify if the memory lock was successful) sudo service elasticsearch start sudo less /var/log/elasticsearch/myCluster.log sudo systemctl enable elasticsearch 6. Test the REST API curl http://localhost:9200 { name : ip-172-31-5-121 , cluster_name : hyperion , cluster_uuid : .... , version : { number : 7.1.0 , build_flavor : default , build_type : deb , build_hash : 606a173 , build_date : 2019-05-16T00:43:15.323135Z , build_snapshot : false, lucene_version : 8.0.0 , minimum_wire_compatibility_version : 6.8.0 , minimum_index_compatibility_version : 6.0.0-beta1 }, tagline : You Know, for Search } The Default user and password is: user: elastic password: changeme You can change the password via the API, like this: curl -X POST localhost:9200/_security/user/elastic/_password?pretty -H 'Content-Type: application/json' -d' { password : new_password }' RabbitMQ Installation Info Follow the detailed installation instructions on the official RabbitMQ documentation 1. Enable the WebUI sudo rabbitmq-plugins enable rabbitmq_management 2. Add vhost sudo rabbitmqctl add_vhost /hyperion 2. Create a user and password sudo rabbitmqctl add_user {my_user} {my_password} 3. Set the user as administrator sudo rabbitmqctl set_user_tags {my_user} administrator 4. Set the user permissions to the vhost sudo rabbitmqctl set_permissions -p /hyperion {my_user} .* .* .* 5. Check access to the WebUI http://localhost:15672 Redis Installation 1. Install sudo apt install redis-server 2. Edit /etc/redis/redis.conf Note By default, redis binds to the localhost address. You need to edit the config file if you want to listen to other network. 3. Change supervised to systemd sudo systemctl restart redis.service NodeJS 1. Install the nodejs source curl -sL https://deb.nodesource.com/setup_12.x | sudo -E bash - 2. Install sudo apt-get install -y nodejs PM2 1. Install sudo npm install pm2@latest -g 2. Run sudo pm2 startup Kibana Installation Note Check for the latest version on the official website 1. Get the binary wget https://artifacts.elastic.co/downloads/kibana/kibana-7.4.0-amd64.deb 2. install sudo apt install ./kibana-7.4.0-amd64.deb 3. Change supervised to systemd sudo systemctl enable kibana 4. Open and test Kibana http://localhost:5601 nodeos config.ini state-history-dir = state-history trace-history = true chain-state-history = true state-history-endpoint = 127.0.0.1:8080 plugin = eosio::state_history_plugin Hyperion Indexer 1. Clone Install packages git clone https://github.com/eosrio/Hyperion-History-API.git cd Hyperion-History-API npm install 2. Edit configs cp example-ecosystem.config.js ecosystem.config.js nano ecosystem.config.js # Enter connection details here (chain name must match on the ecosystem file) cp example-connections.json connections.json nano connections.json connections.json Reference { amqp : { host : 127.0.0.1:5672 , // RabbitMQ Server api : 127.0.0.1:15672 , // RabbitMQ API Endpoint user : username , pass : password , vhost : hyperion // RabbitMQ vhost }, elasticsearch : { host : 127.0.0.1:9200 , // Elasticsearch HTTP API Endpoint user : elastic , pass : password }, redis : { host : 127.0.0.1 , port : 6379 }, chains : { eos : { // Chain name (must match on the ecosystem file) http : http://127.0.0.1:8888 , // Nodeos Chain API Endpoint ship : ws://127.0.0.1:8080 // Nodeos State History Endpoint }, other_chain : {...} } } ecosystem.config.js Reference CHAIN: 'eos', // chain prefix for indexing ABI_CACHE_MODE: 'false', // only cache historical ABIs to redis DEBUG: 'false', // debug mode - display extra logs for debugging LIVE_READER: 'true', // enable continuous reading after reaching the head block FETCH_DELTAS: 'false', // read table deltas CREATE_INDICES: 'v1', // index suffix to be created, set to false to use existing aliases START_ON: 0, // start indexing on block (0=disable) STOP_ON: 0, // stop indexing on block (0=disable) AUTO_STOP: 0, // automatically stop Indexer after X seconds if no more blocks are being processed (0=disable) REWRITE: 'false', // force rewrite the target replay range PURGE_QUEUES: 'false', // clear rabbitmq queues before starting the indexer BATCH_SIZE: 2000, // parallel reader batch size in blocks QUEUE_THRESH: 8000, // queue size limit on rabbitmq LIVE_ONLY: 'false', // only reads realtime data serially FETCH_BLOCK: 'true', // Request full blocks from the state history plugin FETCH_TRACES: 'true', // Request traces from the state history plugin PREVIEW: 'false', // preview mode - prints worker map and exit DISABLE_READING: 'false', // completely disable block reading, for lagged queue processing READERS: 3, // parallel state history readers DESERIALIZERS: 4, // deserialization queues DS_MULT: 4, // deserialization threads per queue ES_IDX_QUEUES: 4, // elastic indexers per queue ES_AD_IDX_QUEUES: 2, // multiplier for action indexing queues READ_PREFETCH: 50, // Stage 1 prefecth size BLOCK_PREFETCH: 5, // Stage 2 prefecth size INDEX_PREFETCH: 500, // Stage 3 prefetch size ENABLE_INDEXING: 'true', // enable elasticsearch indexing INDEX_DELTAS: 'true', // index common table deltas (see delta on definitions/mappings) INDEX_ALL_DELTAS: 'false' // index all table deltas (WARNING) 3. Starting pm2 start --only Indexer --update-env pm2 logs Indexer 4. Stopping Stop reading and wait for queues to flush pm2 trigger Indexer stop Force stop pm2 stop Indexer 5. Starting the API node pm2 start --only API --update-env pm2 logs API Setup Indices and Aliases Load templates first by starting the Hyperion Indexer in preview mode PREVIEW: 'true' Indices and aliases are created automatically using the CREATE_INDICES option (set it to your version suffix e.g, v1, v2, v3) If you want to create them manually, use the commands bellow on the kibana dev console PUT mainnet-action-v1-000001 PUT mainnet-abi-v1-000001 PUT mainnet-block-v1-000001 POST _aliases { actions : [ { add : { index : mainnet-abi-v1-000001 , alias : mainnet-abi } }, { add : { index : mainnet-action-v1-000001 , alias : mainnet-action } }, { add : { index : mainnet-block-v1-000001 , alias : mainnet-block } } ] } Before indexing actions into elasticsearch its required to do a ABI scan pass Start with ABI_CACHE_MODE: 'true', FETCH_BLOCK: 'false', FETCH_TRACES: 'false', INDEX_DELTAS: 'false', INDEX_ALL_DELTAS: 'false', When indexing is finished, change the settings back and restart the indexer. In case you do not have much contract updates, you do not need to run a full pass. Tune your configs to your specific hardware using the following settings: BATCH_SIZE READERS DESERIALIZERS DS_MULT ES_IDX_QUEUES ES_AD_IDX_QUEUES READ_PREFETCH BLOCK_PREFETCH INDEX_PREFETCH API Reference Documentation is automatically generated by Swagger/OpenAPI. Example: OpenAPI Docs","title":"Getting Started"},{"location":"install/#getting-started","text":"","title":"Getting Started"},{"location":"install/#installation","text":"","title":"Installation"},{"location":"install/#dependencies","text":"This setup has only been tested with Ubuntu 18.04, but should work with other OS versions too Elasticsearch 7.4.X RabbitMQ Redis Node.js v12 PM2 Nodeos 1.8.4 w/ state_history_plugin and chain_api_plugin Note The indexer requires redis, pm2 and node.js to be on the same machine. Other dependencies might be installed on other machines, preferably over a very high speed and low latency network. Indexing speed will vary greatly depending on this configuration.","title":"Dependencies"},{"location":"install/#elasticsearch-installation","text":"Info Follow the detailed installation instructions on the official elasticsearch documentation","title":"Elasticsearch Installation"},{"location":"install/#1-edit-etcelasticsearchelasticsearchyml","text":"cluster.name: myCluster bootstrap.memory_lock: true","title":"1. Edit /etc/elasticsearch/elasticsearch.yml"},{"location":"install/#2-edit-etcelasticsearchjvmoptions","text":"# Set your heap size, avoid allocating more than 31GB, even if you have enought RAM. # Test on your specific machine by changing -Xmx32g in the following command: # java -Xmx32g -XX:+UseCompressedOops -XX:+PrintFlagsFinal Oops | grep Oops -Xms16g -Xmx16g","title":"2. Edit /etc/elasticsearch/jvm.options"},{"location":"install/#3-disable-swap","text":"run swapoff -a this will immediately disable swap remove any swap entry from /etc/fstab Tip Identify configured swap devices and files with cat /proc/swaps","title":"3. Disable swap"},{"location":"install/#4-allow-memlock","text":"run sudo systemctl edit elasticsearch and add the following lines [Service] LimitMEMLOCK=infinity","title":"4. Allow memlock"},{"location":"install/#5-start-elasticsearch-and-check-the-logs-verify-if-the-memory-lock-was-successful","text":"sudo service elasticsearch start sudo less /var/log/elasticsearch/myCluster.log sudo systemctl enable elasticsearch","title":"5. Start elasticsearch and check the logs (verify if the memory lock was successful)"},{"location":"install/#6-test-the-rest-api","text":"curl http://localhost:9200 { name : ip-172-31-5-121 , cluster_name : hyperion , cluster_uuid : .... , version : { number : 7.1.0 , build_flavor : default , build_type : deb , build_hash : 606a173 , build_date : 2019-05-16T00:43:15.323135Z , build_snapshot : false, lucene_version : 8.0.0 , minimum_wire_compatibility_version : 6.8.0 , minimum_index_compatibility_version : 6.0.0-beta1 }, tagline : You Know, for Search } The Default user and password is: user: elastic password: changeme You can change the password via the API, like this: curl -X POST localhost:9200/_security/user/elastic/_password?pretty -H 'Content-Type: application/json' -d' { password : new_password }'","title":"6. Test the REST API"},{"location":"install/#rabbitmq-installation","text":"Info Follow the detailed installation instructions on the official RabbitMQ documentation","title":"RabbitMQ Installation"},{"location":"install/#1-enable-the-webui","text":"sudo rabbitmq-plugins enable rabbitmq_management","title":"1. Enable the WebUI"},{"location":"install/#2-add-vhost","text":"sudo rabbitmqctl add_vhost /hyperion","title":"2. Add vhost"},{"location":"install/#2-create-a-user-and-password","text":"sudo rabbitmqctl add_user {my_user} {my_password}","title":"2. Create a user and password"},{"location":"install/#3-set-the-user-as-administrator","text":"sudo rabbitmqctl set_user_tags {my_user} administrator","title":"3. Set the user as administrator"},{"location":"install/#4-set-the-user-permissions-to-the-vhost","text":"sudo rabbitmqctl set_permissions -p /hyperion {my_user} .* .* .*","title":"4. Set the user permissions to the vhost"},{"location":"install/#5-check-access-to-the-webui","text":"http://localhost:15672","title":"5. Check access to the WebUI"},{"location":"install/#redis-installation","text":"","title":"Redis Installation"},{"location":"install/#1-install","text":"sudo apt install redis-server","title":"1. Install"},{"location":"install/#2-edit-etcredisredisconf","text":"Note By default, redis binds to the localhost address. You need to edit the config file if you want to listen to other network.","title":"2. Edit /etc/redis/redis.conf"},{"location":"install/#3-change-supervised-to-systemd","text":"sudo systemctl restart redis.service","title":"3. Change supervised to systemd"},{"location":"install/#nodejs","text":"","title":"NodeJS"},{"location":"install/#1-install-the-nodejs-source","text":"curl -sL https://deb.nodesource.com/setup_12.x | sudo -E bash -","title":"1. Install the nodejs source"},{"location":"install/#2-install","text":"sudo apt-get install -y nodejs","title":"2. Install"},{"location":"install/#pm2","text":"","title":"PM2"},{"location":"install/#1-install_1","text":"sudo npm install pm2@latest -g","title":"1. Install"},{"location":"install/#2-run","text":"sudo pm2 startup","title":"2. Run"},{"location":"install/#kibana-installation","text":"Note Check for the latest version on the official website","title":"Kibana Installation"},{"location":"install/#1-get-the-binary","text":"wget https://artifacts.elastic.co/downloads/kibana/kibana-7.4.0-amd64.deb","title":"1. Get the binary"},{"location":"install/#2-install_1","text":"sudo apt install ./kibana-7.4.0-amd64.deb","title":"2. install"},{"location":"install/#3-change-supervised-to-systemd_1","text":"sudo systemctl enable kibana","title":"3. Change supervised to systemd"},{"location":"install/#4-open-and-test-kibana","text":"http://localhost:5601","title":"4. Open and test Kibana"},{"location":"install/#nodeos-configini","text":"state-history-dir = state-history trace-history = true chain-state-history = true state-history-endpoint = 127.0.0.1:8080 plugin = eosio::state_history_plugin","title":"nodeos config.ini"},{"location":"install/#hyperion-indexer","text":"","title":"Hyperion Indexer"},{"location":"install/#1-clone-install-packages","text":"git clone https://github.com/eosrio/Hyperion-History-API.git cd Hyperion-History-API npm install","title":"1. Clone &amp; Install packages"},{"location":"install/#2-edit-configs","text":"cp example-ecosystem.config.js ecosystem.config.js nano ecosystem.config.js # Enter connection details here (chain name must match on the ecosystem file) cp example-connections.json connections.json nano connections.json connections.json Reference { amqp : { host : 127.0.0.1:5672 , // RabbitMQ Server api : 127.0.0.1:15672 , // RabbitMQ API Endpoint user : username , pass : password , vhost : hyperion // RabbitMQ vhost }, elasticsearch : { host : 127.0.0.1:9200 , // Elasticsearch HTTP API Endpoint user : elastic , pass : password }, redis : { host : 127.0.0.1 , port : 6379 }, chains : { eos : { // Chain name (must match on the ecosystem file) http : http://127.0.0.1:8888 , // Nodeos Chain API Endpoint ship : ws://127.0.0.1:8080 // Nodeos State History Endpoint }, other_chain : {...} } } ecosystem.config.js Reference CHAIN: 'eos', // chain prefix for indexing ABI_CACHE_MODE: 'false', // only cache historical ABIs to redis DEBUG: 'false', // debug mode - display extra logs for debugging LIVE_READER: 'true', // enable continuous reading after reaching the head block FETCH_DELTAS: 'false', // read table deltas CREATE_INDICES: 'v1', // index suffix to be created, set to false to use existing aliases START_ON: 0, // start indexing on block (0=disable) STOP_ON: 0, // stop indexing on block (0=disable) AUTO_STOP: 0, // automatically stop Indexer after X seconds if no more blocks are being processed (0=disable) REWRITE: 'false', // force rewrite the target replay range PURGE_QUEUES: 'false', // clear rabbitmq queues before starting the indexer BATCH_SIZE: 2000, // parallel reader batch size in blocks QUEUE_THRESH: 8000, // queue size limit on rabbitmq LIVE_ONLY: 'false', // only reads realtime data serially FETCH_BLOCK: 'true', // Request full blocks from the state history plugin FETCH_TRACES: 'true', // Request traces from the state history plugin PREVIEW: 'false', // preview mode - prints worker map and exit DISABLE_READING: 'false', // completely disable block reading, for lagged queue processing READERS: 3, // parallel state history readers DESERIALIZERS: 4, // deserialization queues DS_MULT: 4, // deserialization threads per queue ES_IDX_QUEUES: 4, // elastic indexers per queue ES_AD_IDX_QUEUES: 2, // multiplier for action indexing queues READ_PREFETCH: 50, // Stage 1 prefecth size BLOCK_PREFETCH: 5, // Stage 2 prefecth size INDEX_PREFETCH: 500, // Stage 3 prefetch size ENABLE_INDEXING: 'true', // enable elasticsearch indexing INDEX_DELTAS: 'true', // index common table deltas (see delta on definitions/mappings) INDEX_ALL_DELTAS: 'false' // index all table deltas (WARNING)","title":"2. Edit configs"},{"location":"install/#3-starting","text":"pm2 start --only Indexer --update-env pm2 logs Indexer","title":"3. Starting"},{"location":"install/#4-stopping","text":"Stop reading and wait for queues to flush pm2 trigger Indexer stop Force stop pm2 stop Indexer","title":"4. Stopping"},{"location":"install/#5-starting-the-api-node","text":"pm2 start --only API --update-env pm2 logs API","title":"5. Starting the API node"},{"location":"install/#setup-indices-and-aliases","text":"Load templates first by starting the Hyperion Indexer in preview mode PREVIEW: 'true' Indices and aliases are created automatically using the CREATE_INDICES option (set it to your version suffix e.g, v1, v2, v3) If you want to create them manually, use the commands bellow on the kibana dev console PUT mainnet-action-v1-000001 PUT mainnet-abi-v1-000001 PUT mainnet-block-v1-000001 POST _aliases { actions : [ { add : { index : mainnet-abi-v1-000001 , alias : mainnet-abi } }, { add : { index : mainnet-action-v1-000001 , alias : mainnet-action } }, { add : { index : mainnet-block-v1-000001 , alias : mainnet-block } } ] } Before indexing actions into elasticsearch its required to do a ABI scan pass Start with ABI_CACHE_MODE: 'true', FETCH_BLOCK: 'false', FETCH_TRACES: 'false', INDEX_DELTAS: 'false', INDEX_ALL_DELTAS: 'false', When indexing is finished, change the settings back and restart the indexer. In case you do not have much contract updates, you do not need to run a full pass. Tune your configs to your specific hardware using the following settings: BATCH_SIZE READERS DESERIALIZERS DS_MULT ES_IDX_QUEUES ES_AD_IDX_QUEUES READ_PREFETCH BLOCK_PREFETCH INDEX_PREFETCH","title":"Setup Indices and Aliases"},{"location":"install/#api-reference","text":"Documentation is automatically generated by Swagger/OpenAPI. Example: OpenAPI Docs","title":"API Reference"},{"location":"roadmap/","text":"Roadmap Table deltas storage queries (in progress) Real-time streaming support (in progress) Plugin system (in progress) Control GUI","title":"Roadmap"},{"location":"roadmap/#roadmap","text":"Table deltas storage queries (in progress) Real-time streaming support (in progress) Plugin system (in progress) Control GUI","title":"Roadmap"}]}