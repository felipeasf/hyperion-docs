{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Hyperion History API Scalable Full History API Solution for EOSIO based blockchains Made with \u2665 by EOS Rio Introducing an storage-optimized action format for EOSIO The original history_plugin bundled with eosio, that provided the v1 api, stored inline action traces nested inside their root actions. This led to an excessive amount of data being stored and also transferred whenever a user requested the action history for a given account. Also inline actions are used as a \"event\" mechanism to notify parties on a transaction. Based on those Hyperion implements some changes actions are stored in a flattened format a parent field is added to the inline actions to point to the parent global sequence if the inline action data is identical to the parent it is considered a notification and thus removed from the database no blocks or transaction data is stored, all information can be reconstructed from actions With those changes the API format focus on delivering faster search times, lower bandwidth overhead and easier usability for UI/UX developers.","title":"Home"},{"location":"#hyperion-history-api","text":"Scalable Full History API Solution for EOSIO based blockchains Made with \u2665 by EOS Rio","title":"Hyperion History API"},{"location":"#introducing-an-storage-optimized-action-format-for-eosio","text":"The original history_plugin bundled with eosio, that provided the v1 api, stored inline action traces nested inside their root actions. This led to an excessive amount of data being stored and also transferred whenever a user requested the action history for a given account. Also inline actions are used as a \"event\" mechanism to notify parties on a transaction. Based on those Hyperion implements some changes actions are stored in a flattened format a parent field is added to the inline actions to point to the parent global sequence if the inline action data is identical to the parent it is considered a notification and thus removed from the database no blocks or transaction data is stored, all information can be reconstructed from actions With those changes the API format focus on delivering faster search times, lower bandwidth overhead and easier usability for UI/UX developers.","title":"Introducing an storage-optimized action format for EOSIO"},{"location":"data/","text":"Action Data Structure (work in progress) @timestamp - block time global_sequence - unique action global_sequence, used as index id parent - points to the parent action (in the case of an inline action) or equal to 0 if root level block_num - block number where the action was processed trx_id - transaction id producer - block producer act account - contract account name - contract method name authorization - array of signers actor - signing actor permission - signing permission data - action data input object account_ram_deltas - array of ram deltas and payers account delta notified - array of accounts that were notified (via inline action events)","title":"Data Structure"},{"location":"data/#action-data-structure-work-in-progress","text":"@timestamp - block time global_sequence - unique action global_sequence, used as index id parent - points to the parent action (in the case of an inline action) or equal to 0 if root level block_num - block number where the action was processed trx_id - transaction id producer - block producer act account - contract account name - contract method name authorization - array of signers actor - signing actor permission - signing permission data - action data input object account_ram_deltas - array of ram deltas and payers account delta notified - array of accounts that were notified (via inline action events)","title":"Action Data Structure (work in progress)"},{"location":"docker/","text":"Hyperion Docker Dependencies docker and docker-compose INSTALL Edit connections.json file and change http://host.docker.internal:8888 and ws://host.docker.internal:8080 for the nodeos address Change passwords in docker-compose.yml file Run docker-compose up Usage Kibana Access http://localhost:5601/ RabbitMQ Access http://127.0.0.1:15672/ Hyperion API Access http://127.0.0.1:7000/v2/history/get_actions or curl http://127.0.0.1:7000/v2/history/get_actions Troubleshooting If you're having problems accesing Kibana or using elasticsearch api, you could disable the xpack security on the docker-compose.yml setting it to false: xpack.security.enabled=false","title":"Docker"},{"location":"docker/#hyperion-docker","text":"","title":"Hyperion Docker"},{"location":"docker/#dependencies","text":"docker and docker-compose","title":"Dependencies"},{"location":"docker/#install","text":"Edit connections.json file and change http://host.docker.internal:8888 and ws://host.docker.internal:8080 for the nodeos address Change passwords in docker-compose.yml file Run docker-compose up","title":"INSTALL"},{"location":"docker/#usage","text":"","title":"Usage"},{"location":"docker/#kibana","text":"Access http://localhost:5601/","title":"Kibana"},{"location":"docker/#rabbitmq","text":"Access http://127.0.0.1:15672/","title":"RabbitMQ"},{"location":"docker/#hyperion-api","text":"Access http://127.0.0.1:7000/v2/history/get_actions or curl http://127.0.0.1:7000/v2/history/get_actions","title":"Hyperion API"},{"location":"docker/#troubleshooting","text":"If you're having problems accesing Kibana or using elasticsearch api, you could disable the xpack security on the docker-compose.yml setting it to false: xpack.security.enabled=false","title":"Troubleshooting"},{"location":"install/","text":"Getting Started Installation Dependencies This setup has only been tested with Ubuntu 18.04, but should work with other OS versions too Elasticsearch 7.4.X RabbitMQ Redis Node.js v12 PM2 Nodeos 1.8.4 w/ state_history_plugin and chain_api_plugin Note The indexer requires redis, pm2 and node.js to be on the same machine. Other dependencies might be installed on other machines, preferably over a very high speed and low latency network. Indexing speed will vary greatly depending on this configuration. Elasticsearch Installation Info Follow the detailed installation instructions on the official elasticsearch documentation 1. Edit /etc/elasticsearch/elasticsearch.yml cluster.name: myCluster bootstrap.memory_lock: true 2. Edit /etc/elasticsearch/jvm.options # Set your heap size, avoid allocating more than 31GB, even if you have enought RAM. # Test on your specific machine by changing -Xmx32g in the following command: # java -Xmx32g -XX:+UseCompressedOops -XX:+PrintFlagsFinal Oops | grep Oops -Xms16g -Xmx16g 3. Disable swap run swapoff -a this will immediately disable swap remove any swap entry from /etc/fstab Tip Identify configured swap devices and files with cat /proc/swaps 4. Allow memlock run sudo systemctl edit elasticsearch and add the following lines [Service] LimitMEMLOCK=infinity 5. Start elasticsearch and check the logs (verify if the memory lock was successful) sudo service elasticsearch start sudo less /var/log/elasticsearch/myCluster.log sudo systemctl enable elasticsearch 6. Test the REST API curl http://localhost:9200 { name : ip-172-31-5-121 , cluster_name : hyperion , cluster_uuid : .... , version : { number : 7.1.0 , build_flavor : default , build_type : deb , build_hash : 606a173 , build_date : 2019-05-16T00:43:15.323135Z , build_snapshot : false, lucene_version : 8.0.0 , minimum_wire_compatibility_version : 6.8.0 , minimum_index_compatibility_version : 6.0.0-beta1 }, tagline : You Know, for Search } The Default user and password is: user: elastic password: changeme You can change the password via the API, like this: curl -X POST localhost:9200/_security/user/elastic/_password?pretty -H 'Content-Type: application/json' -d' { password : new_password }' RabbitMQ Installation Info Follow the detailed installation instructions on the official RabbitMQ documentation 1. Enable the WebUI sudo rabbitmq-plugins enable rabbitmq_management 2. Add vhost sudo rabbitmqctl add_vhost /hyperion 2. Create a user and password sudo rabbitmqctl add_user {my_user} {my_password} 3. Set the user as administrator sudo rabbitmqctl set_user_tags {my_user} administrator 4. Set the user permissions to the vhost sudo rabbitmqctl set_permissions -p /hyperion {my_user} .* .* .* 5. Check access to the WebUI http://localhost:15672 Redis Installation 1. Install sudo apt install redis-server 2. Edit /etc/redis/redis.conf Note By default, redis binds to the localhost address. You need to edit the config file if you want to listen to other network. 3. Change supervised to systemd sudo systemctl restart redis.service NodeJS 1. Install the nodejs source curl -sL https://deb.nodesource.com/setup_12.x | sudo -E bash - 2. Install sudo apt-get install -y nodejs PM2 1. Install sudo npm install pm2@latest -g 2. Run sudo pm2 startup Kibana Installation Note Check for the latest version on the official website 1. Get the binary wget https://artifacts.elastic.co/downloads/kibana/kibana-7.4.0-amd64.deb 2. install sudo apt install ./kibana-7.4.0-amd64.deb 3. Change supervised to systemd sudo systemctl enable kibana 4. Open and test Kibana http://localhost:5601 nodeos config.ini state-history-dir = state-history trace-history = true chain-state-history = true state-history-endpoint = 127.0.0.1:8080 plugin = eosio::state_history_plugin Hyperion Indexer 1. Clone Install packages git clone https://github.com/eosrio/Hyperion-History-API.git cd Hyperion-History-API npm install 2. Edit configs cp example-ecosystem.config.js ecosystem.config.js nano ecosystem.config.js # Enter connection details here (chain name must match on the ecosystem file) cp example-connections.json connections.json nano connections.json connections.json Reference { amqp : { host : 127.0.0.1:5672 , // RabbitMQ Server api : 127.0.0.1:15672 , // RabbitMQ API Endpoint user : username , pass : password , vhost : hyperion // RabbitMQ vhost }, elasticsearch : { host : 127.0.0.1:9200 , // Elasticsearch HTTP API Endpoint ingest_nodes : [ hyperion-elastic:9200 ], user : elastic , pass : password }, redis : { host : 127.0.0.1 , port : 6379 }, chains : { eos : { // Chain name (must match on the ecosystem file) name : EOS Mainnet , chain_id : aca376f206b8fc25a6ed44dbdc66547c36c6c33e3a119ffbeaef943642f0e906 , http : http://127.0.0.1:8888 , // Nodeos Chain API Endpoint ship : ws://127.0.0.1:8080 , // Nodeos State History Endpoint WS_ROUTER_PORT : 7001 }, other_chain : {...} } } ecosystem.config.js Reference CHAIN: 'eos', // chain prefix for indexing ABI_CACHE_MODE: 'false', // only cache historical ABIs to redis DEBUG: 'false', // debug mode - display extra logs for debugging LIVE_READER: 'true', // enable continuous reading after reaching the head block FETCH_DELTAS: 'false', // read table deltas CREATE_INDICES: 'v1', // index suffix to be created, set to false to use existing aliases START_ON: 0, // start indexing on block (0=disable) STOP_ON: 0, // stop indexing on block (0=disable) AUTO_STOP: 0, // automatically stop Indexer after X seconds if no more blocks are being processed (0=disable) REWRITE: 'false', // force rewrite the target replay range PURGE_QUEUES: 'false', // clear rabbitmq queues before starting the indexer BATCH_SIZE: 2000, // parallel reader batch size in blocks QUEUE_THRESH: 8000, // queue size limit on rabbitmq LIVE_ONLY: 'false', // only reads realtime data serially FETCH_BLOCK: 'true', // Request full blocks from the state history plugin FETCH_TRACES: 'true', // Request traces from the state history plugin PREVIEW: 'false', // preview mode - prints worker map and exit DISABLE_READING: 'false', // completely disable block reading, for lagged queue processing READERS: 3, // parallel state history readers DESERIALIZERS: 4, // deserialization queues DS_MULT: 4, // deserialization threads per queue ES_IDX_QUEUES: 4, // elastic indexers per queue ES_AD_IDX_QUEUES: 2, // multiplier for action indexing queues READ_PREFETCH: 50, // Stage 1 prefecth size BLOCK_PREFETCH: 5, // Stage 2 prefecth size INDEX_PREFETCH: 500, // Stage 3 prefetch size ENABLE_INDEXING: 'true', // enable elasticsearch indexing INDEX_DELTAS: 'true', // index common table deltas (see delta on definitions/mappings) INDEX_ALL_DELTAS: 'false' // index all table deltas (WARNING) Setup Indices and Aliases Load templates first by starting the Hyperion Indexer in preview mode PREVIEW: 'true' Indices and aliases are created automatically using the CREATE_INDICES option (set it to your version suffix e.g, v1, v2, v3) If you want to create them manually, use the commands bellow on the kibana dev console PUT mainnet-action-v1-000001 PUT mainnet-abi-v1-000001 PUT mainnet-block-v1-000001 POST _aliases { actions : [ { add : { index : mainnet-abi-v1-000001 , alias : mainnet-abi } }, { add : { index : mainnet-action-v1-000001 , alias : mainnet-action } }, { add : { index : mainnet-block-v1-000001 , alias : mainnet-block } } ] } Before indexing actions into elasticsearch its required to do a ABI scan pass Start with ABI_CACHE_MODE: 'true', FETCH_BLOCK: 'false', FETCH_TRACES: 'false', INDEX_DELTAS: 'false', INDEX_ALL_DELTAS: 'false', When indexing is finished, change the settings back and restart the indexer. In case you do not have much contract updates, you do not need to run a full pass. Tune your configs to your specific hardware using the following settings: BATCH_SIZE READERS DESERIALIZERS DS_MULT ES_IDX_QUEUES ES_AD_IDX_QUEUES READ_PREFETCH BLOCK_PREFETCH INDEX_PREFETCH Start and Stop Start indexing pm2 start --only Indexer --update-env pm2 logs Indexer Stop reading and wait for queues to flush pm2 trigger Indexer stop Force stop pm2 stop Indexer Starting the API node pm2 start --only API --update-env pm2 logs API API Reference Documentation is automatically generated by Swagger/OpenAPI. Example: OpenAPI Docs","title":"Getting Started"},{"location":"install/#getting-started","text":"","title":"Getting Started"},{"location":"install/#installation","text":"","title":"Installation"},{"location":"install/#dependencies","text":"This setup has only been tested with Ubuntu 18.04, but should work with other OS versions too Elasticsearch 7.4.X RabbitMQ Redis Node.js v12 PM2 Nodeos 1.8.4 w/ state_history_plugin and chain_api_plugin Note The indexer requires redis, pm2 and node.js to be on the same machine. Other dependencies might be installed on other machines, preferably over a very high speed and low latency network. Indexing speed will vary greatly depending on this configuration.","title":"Dependencies"},{"location":"install/#elasticsearch-installation","text":"Info Follow the detailed installation instructions on the official elasticsearch documentation","title":"Elasticsearch Installation"},{"location":"install/#1-edit-etcelasticsearchelasticsearchyml","text":"cluster.name: myCluster bootstrap.memory_lock: true","title":"1. Edit /etc/elasticsearch/elasticsearch.yml"},{"location":"install/#2-edit-etcelasticsearchjvmoptions","text":"# Set your heap size, avoid allocating more than 31GB, even if you have enought RAM. # Test on your specific machine by changing -Xmx32g in the following command: # java -Xmx32g -XX:+UseCompressedOops -XX:+PrintFlagsFinal Oops | grep Oops -Xms16g -Xmx16g","title":"2. Edit /etc/elasticsearch/jvm.options"},{"location":"install/#3-disable-swap","text":"run swapoff -a this will immediately disable swap remove any swap entry from /etc/fstab Tip Identify configured swap devices and files with cat /proc/swaps","title":"3. Disable swap"},{"location":"install/#4-allow-memlock","text":"run sudo systemctl edit elasticsearch and add the following lines [Service] LimitMEMLOCK=infinity","title":"4. Allow memlock"},{"location":"install/#5-start-elasticsearch-and-check-the-logs-verify-if-the-memory-lock-was-successful","text":"sudo service elasticsearch start sudo less /var/log/elasticsearch/myCluster.log sudo systemctl enable elasticsearch","title":"5. Start elasticsearch and check the logs (verify if the memory lock was successful)"},{"location":"install/#6-test-the-rest-api","text":"curl http://localhost:9200 { name : ip-172-31-5-121 , cluster_name : hyperion , cluster_uuid : .... , version : { number : 7.1.0 , build_flavor : default , build_type : deb , build_hash : 606a173 , build_date : 2019-05-16T00:43:15.323135Z , build_snapshot : false, lucene_version : 8.0.0 , minimum_wire_compatibility_version : 6.8.0 , minimum_index_compatibility_version : 6.0.0-beta1 }, tagline : You Know, for Search } The Default user and password is: user: elastic password: changeme You can change the password via the API, like this: curl -X POST localhost:9200/_security/user/elastic/_password?pretty -H 'Content-Type: application/json' -d' { password : new_password }'","title":"6. Test the REST API"},{"location":"install/#rabbitmq-installation","text":"Info Follow the detailed installation instructions on the official RabbitMQ documentation","title":"RabbitMQ Installation"},{"location":"install/#1-enable-the-webui","text":"sudo rabbitmq-plugins enable rabbitmq_management","title":"1. Enable the WebUI"},{"location":"install/#2-add-vhost","text":"sudo rabbitmqctl add_vhost /hyperion","title":"2. Add vhost"},{"location":"install/#2-create-a-user-and-password","text":"sudo rabbitmqctl add_user {my_user} {my_password}","title":"2. Create a user and password"},{"location":"install/#3-set-the-user-as-administrator","text":"sudo rabbitmqctl set_user_tags {my_user} administrator","title":"3. Set the user as administrator"},{"location":"install/#4-set-the-user-permissions-to-the-vhost","text":"sudo rabbitmqctl set_permissions -p /hyperion {my_user} .* .* .*","title":"4. Set the user permissions to the vhost"},{"location":"install/#5-check-access-to-the-webui","text":"http://localhost:15672","title":"5. Check access to the WebUI"},{"location":"install/#redis-installation","text":"","title":"Redis Installation"},{"location":"install/#1-install","text":"sudo apt install redis-server","title":"1. Install"},{"location":"install/#2-edit-etcredisredisconf","text":"Note By default, redis binds to the localhost address. You need to edit the config file if you want to listen to other network.","title":"2. Edit /etc/redis/redis.conf"},{"location":"install/#3-change-supervised-to-systemd","text":"sudo systemctl restart redis.service","title":"3. Change supervised to systemd"},{"location":"install/#nodejs","text":"","title":"NodeJS"},{"location":"install/#1-install-the-nodejs-source","text":"curl -sL https://deb.nodesource.com/setup_12.x | sudo -E bash -","title":"1. Install the nodejs source"},{"location":"install/#2-install","text":"sudo apt-get install -y nodejs","title":"2. Install"},{"location":"install/#pm2","text":"","title":"PM2"},{"location":"install/#1-install_1","text":"sudo npm install pm2@latest -g","title":"1. Install"},{"location":"install/#2-run","text":"sudo pm2 startup","title":"2. Run"},{"location":"install/#kibana-installation","text":"Note Check for the latest version on the official website","title":"Kibana Installation"},{"location":"install/#1-get-the-binary","text":"wget https://artifacts.elastic.co/downloads/kibana/kibana-7.4.0-amd64.deb","title":"1. Get the binary"},{"location":"install/#2-install_1","text":"sudo apt install ./kibana-7.4.0-amd64.deb","title":"2. install"},{"location":"install/#3-change-supervised-to-systemd_1","text":"sudo systemctl enable kibana","title":"3. Change supervised to systemd"},{"location":"install/#4-open-and-test-kibana","text":"http://localhost:5601","title":"4. Open and test Kibana"},{"location":"install/#nodeos-configini","text":"state-history-dir = state-history trace-history = true chain-state-history = true state-history-endpoint = 127.0.0.1:8080 plugin = eosio::state_history_plugin","title":"nodeos config.ini"},{"location":"install/#hyperion-indexer","text":"","title":"Hyperion Indexer"},{"location":"install/#1-clone-install-packages","text":"git clone https://github.com/eosrio/Hyperion-History-API.git cd Hyperion-History-API npm install","title":"1. Clone &amp; Install packages"},{"location":"install/#2-edit-configs","text":"cp example-ecosystem.config.js ecosystem.config.js nano ecosystem.config.js # Enter connection details here (chain name must match on the ecosystem file) cp example-connections.json connections.json nano connections.json connections.json Reference { amqp : { host : 127.0.0.1:5672 , // RabbitMQ Server api : 127.0.0.1:15672 , // RabbitMQ API Endpoint user : username , pass : password , vhost : hyperion // RabbitMQ vhost }, elasticsearch : { host : 127.0.0.1:9200 , // Elasticsearch HTTP API Endpoint ingest_nodes : [ hyperion-elastic:9200 ], user : elastic , pass : password }, redis : { host : 127.0.0.1 , port : 6379 }, chains : { eos : { // Chain name (must match on the ecosystem file) name : EOS Mainnet , chain_id : aca376f206b8fc25a6ed44dbdc66547c36c6c33e3a119ffbeaef943642f0e906 , http : http://127.0.0.1:8888 , // Nodeos Chain API Endpoint ship : ws://127.0.0.1:8080 , // Nodeos State History Endpoint WS_ROUTER_PORT : 7001 }, other_chain : {...} } } ecosystem.config.js Reference CHAIN: 'eos', // chain prefix for indexing ABI_CACHE_MODE: 'false', // only cache historical ABIs to redis DEBUG: 'false', // debug mode - display extra logs for debugging LIVE_READER: 'true', // enable continuous reading after reaching the head block FETCH_DELTAS: 'false', // read table deltas CREATE_INDICES: 'v1', // index suffix to be created, set to false to use existing aliases START_ON: 0, // start indexing on block (0=disable) STOP_ON: 0, // stop indexing on block (0=disable) AUTO_STOP: 0, // automatically stop Indexer after X seconds if no more blocks are being processed (0=disable) REWRITE: 'false', // force rewrite the target replay range PURGE_QUEUES: 'false', // clear rabbitmq queues before starting the indexer BATCH_SIZE: 2000, // parallel reader batch size in blocks QUEUE_THRESH: 8000, // queue size limit on rabbitmq LIVE_ONLY: 'false', // only reads realtime data serially FETCH_BLOCK: 'true', // Request full blocks from the state history plugin FETCH_TRACES: 'true', // Request traces from the state history plugin PREVIEW: 'false', // preview mode - prints worker map and exit DISABLE_READING: 'false', // completely disable block reading, for lagged queue processing READERS: 3, // parallel state history readers DESERIALIZERS: 4, // deserialization queues DS_MULT: 4, // deserialization threads per queue ES_IDX_QUEUES: 4, // elastic indexers per queue ES_AD_IDX_QUEUES: 2, // multiplier for action indexing queues READ_PREFETCH: 50, // Stage 1 prefecth size BLOCK_PREFETCH: 5, // Stage 2 prefecth size INDEX_PREFETCH: 500, // Stage 3 prefetch size ENABLE_INDEXING: 'true', // enable elasticsearch indexing INDEX_DELTAS: 'true', // index common table deltas (see delta on definitions/mappings) INDEX_ALL_DELTAS: 'false' // index all table deltas (WARNING)","title":"2. Edit configs"},{"location":"install/#setup-indices-and-aliases","text":"Load templates first by starting the Hyperion Indexer in preview mode PREVIEW: 'true' Indices and aliases are created automatically using the CREATE_INDICES option (set it to your version suffix e.g, v1, v2, v3) If you want to create them manually, use the commands bellow on the kibana dev console PUT mainnet-action-v1-000001 PUT mainnet-abi-v1-000001 PUT mainnet-block-v1-000001 POST _aliases { actions : [ { add : { index : mainnet-abi-v1-000001 , alias : mainnet-abi } }, { add : { index : mainnet-action-v1-000001 , alias : mainnet-action } }, { add : { index : mainnet-block-v1-000001 , alias : mainnet-block } } ] } Before indexing actions into elasticsearch its required to do a ABI scan pass Start with ABI_CACHE_MODE: 'true', FETCH_BLOCK: 'false', FETCH_TRACES: 'false', INDEX_DELTAS: 'false', INDEX_ALL_DELTAS: 'false', When indexing is finished, change the settings back and restart the indexer. In case you do not have much contract updates, you do not need to run a full pass. Tune your configs to your specific hardware using the following settings: BATCH_SIZE READERS DESERIALIZERS DS_MULT ES_IDX_QUEUES ES_AD_IDX_QUEUES READ_PREFETCH BLOCK_PREFETCH INDEX_PREFETCH","title":"Setup Indices and Aliases"},{"location":"install/#start-and-stop","text":"Start indexing pm2 start --only Indexer --update-env pm2 logs Indexer Stop reading and wait for queues to flush pm2 trigger Indexer stop Force stop pm2 stop Indexer Starting the API node pm2 start --only API --update-env pm2 logs API","title":"Start and Stop"},{"location":"install/#api-reference","text":"Documentation is automatically generated by Swagger/OpenAPI. Example: OpenAPI Docs","title":"API Reference"},{"location":"kibana/","text":"The purpose here is to guide you through some basic steps using and configuring the Kibana. For more detailed information, please, refer to the official documentation . Running Kibana with systemd To configure Kibana to start automatically when the system boots up, run the following commands: sudo /bin/systemctl daemon-reload sudo /bin/systemctl enable kibana.service Kibana can be started and stopped as follows: sudo systemctl start kibana.service sudo systemctl stop kibana.service These commands provide no feedback as to whether Kibana was started successfully or not. Log information can be accessed via journalctl -u kibana.service. Opening Kibana Open http://localhost:5601 and check if you can access Kibana. If Kibana asks for credentials, the default user and password is: user: elastic password: changeme If you can't access, check your credentials on your config file. Creating index pattern To create a new index pattern, go to Management Click on Index Patterns at the left menu Click on Create index pattern Index Management Discover","title":"Kibana"},{"location":"kibana/#running-kibana-with-systemd","text":"To configure Kibana to start automatically when the system boots up, run the following commands: sudo /bin/systemctl daemon-reload sudo /bin/systemctl enable kibana.service Kibana can be started and stopped as follows: sudo systemctl start kibana.service sudo systemctl stop kibana.service These commands provide no feedback as to whether Kibana was started successfully or not. Log information can be accessed via journalctl -u kibana.service.","title":"Running Kibana with systemd"},{"location":"kibana/#opening-kibana","text":"Open http://localhost:5601 and check if you can access Kibana. If Kibana asks for credentials, the default user and password is: user: elastic password: changeme If you can't access, check your credentials on your config file.","title":"Opening Kibana"},{"location":"kibana/#creating-index-pattern","text":"To create a new index pattern, go to Management Click on Index Patterns at the left menu Click on Create index pattern","title":"Creating index pattern"},{"location":"kibana/#index-management","text":"","title":"Index Management"},{"location":"kibana/#discover","text":"","title":"Discover"},{"location":"queries/","text":"","title":"Queries"},{"location":"roadmap/","text":"Roadmap Table deltas storage queries (in progress) Real-time streaming support (in progress) Plugin system (in progress) Control GUI","title":"Roadmap"},{"location":"roadmap/#roadmap","text":"Table deltas storage queries (in progress) Real-time streaming support (in progress) Plugin system (in progress) Control GUI","title":"Roadmap"},{"location":"stream_client/","text":"Hyperion Stream Client Live Streaming client for Hyperion History API Instalation Clone this repository Run npm install Configure the client Run the client Configuration 1. Connection Setup the endpoint that you want to fetch data const client = new Hyperion({endpoint}}, { async: false }); {Explain async!!!} Example: const client = new Hyperion('https://daobet.eosrio.io'}, { async: false }); When you successfully connects to the hyperion, you'll receive a handshake message: { event: 'handshake', chain: '{chain_name}' } 2. Stream Data Structure Setup the data structure to be streamed contract - contract account action - action name account - account name start_from - start reading on block (0=disable) read_until - stop reading on block (0=disable) filters - actions filter (more details below) Example: client.streamActions({ contract: 'eosio.token', action: 'transfer', account: 'eosriobrazil', start_from: 1, read_until: 0, filters: [], }); 2. Filters You can setup filters to refine your stream. For example, to filter the stream above for every transfers made to eosriobrazil account: client.streamActions({ contract: 'eosio.token', action: 'transfer', account: 'eosriobrazil', start_from: 1, read_until: 0, filters: [ {field: '@transfer.to', value: 'eosriobrazil'} ], }); To refine even more your stream, you could add more filters. Remember that adding more filters will result in a AND operation, by now it's not possible to make OR operations with filters. Example: client.streamActions({ contract: 'eosio.token', action: 'transfer', account: 'eosriobrazil', start_from: 1, read_until: 0, filters: [ {field: '@transfer.from', value: 'eosio'} {field: '@transfer.to', value: 'eosriobrazil'} ], }); {ADICIONAR LISTA DE FILTROS??} 3. Handling Data To handle data from queues, client.onData = async (data, ack) = { {code here} } Example: client.onData = async (data, ack) = { const act = data['act']; console.log(`\\n---- ACTION - ${data['global_sequence']} | BLOCK: ${data['block_num']} | ${act.account}::${act.name}`); for (const key in act.data) { if (act.data.hasOwnProperty(key)) { console.log(`${key} = ${act.data[key]}`); } } 4. Usage Examples ???","title":"Stream Client"},{"location":"stream_client/#hyperion-stream-client","text":"Live Streaming client for Hyperion History API","title":"Hyperion Stream Client"},{"location":"stream_client/#instalation","text":"Clone this repository Run npm install Configure the client Run the client","title":"Instalation"},{"location":"stream_client/#configuration","text":"","title":"Configuration"},{"location":"stream_client/#1-connection","text":"Setup the endpoint that you want to fetch data const client = new Hyperion({endpoint}}, { async: false }); {Explain async!!!} Example: const client = new Hyperion('https://daobet.eosrio.io'}, { async: false }); When you successfully connects to the hyperion, you'll receive a handshake message: { event: 'handshake', chain: '{chain_name}' }","title":"1. Connection"},{"location":"stream_client/#2-stream-data-structure","text":"Setup the data structure to be streamed contract - contract account action - action name account - account name start_from - start reading on block (0=disable) read_until - stop reading on block (0=disable) filters - actions filter (more details below) Example: client.streamActions({ contract: 'eosio.token', action: 'transfer', account: 'eosriobrazil', start_from: 1, read_until: 0, filters: [], });","title":"2. Stream Data Structure"},{"location":"stream_client/#2-filters","text":"You can setup filters to refine your stream. For example, to filter the stream above for every transfers made to eosriobrazil account: client.streamActions({ contract: 'eosio.token', action: 'transfer', account: 'eosriobrazil', start_from: 1, read_until: 0, filters: [ {field: '@transfer.to', value: 'eosriobrazil'} ], }); To refine even more your stream, you could add more filters. Remember that adding more filters will result in a AND operation, by now it's not possible to make OR operations with filters. Example: client.streamActions({ contract: 'eosio.token', action: 'transfer', account: 'eosriobrazil', start_from: 1, read_until: 0, filters: [ {field: '@transfer.from', value: 'eosio'} {field: '@transfer.to', value: 'eosriobrazil'} ], }); {ADICIONAR LISTA DE FILTROS??}","title":"2. Filters"},{"location":"stream_client/#3-handling-data","text":"To handle data from queues, client.onData = async (data, ack) = { {code here} } Example: client.onData = async (data, ack) = { const act = data['act']; console.log(`\\n---- ACTION - ${data['global_sequence']} | BLOCK: ${data['block_num']} | ${act.account}::${act.name}`); for (const key in act.data) { if (act.data.hasOwnProperty(key)) { console.log(`${key} = ${act.data[key]}`); } }","title":"3. Handling Data"},{"location":"stream_client/#4-usage-examples","text":"???","title":"4. Usage Examples"},{"location":"system_req/","text":"Minimum Required Specifications OS: Windows 7/8.1/10 \u2013 64-Bit CPU: AMD Phenom II X4 965, Intel Core i3-2100, or equivalent Memory: 8GB Graphics Card: AMD Radeon HD 7850 2GB, NVIDIA GTX 660 2GB, or equivalent Hard Drive: At least 50 GB of free space Online Connection Requirements: 512kbps minimum internet speed. Internet Connection required to install and play. Recommended Specifications OS: Windows 10 \u2013 64-Bit CPU: AMD Athlon X4 870K, Intel i3 6300T or equivalent Memory: 8GB Graphics Card: AMD Radeon R9 270X, NVIDIA GeForce GTX 670, or equivalent Hard Drive: At least 50 GB of free space Online Connection Requirements: Broadband connection recommended. Internet Connection required to install and play.","title":"System Requirements"},{"location":"system_req/#minimum-required-specifications","text":"OS: Windows 7/8.1/10 \u2013 64-Bit CPU: AMD Phenom II X4 965, Intel Core i3-2100, or equivalent Memory: 8GB Graphics Card: AMD Radeon HD 7850 2GB, NVIDIA GTX 660 2GB, or equivalent Hard Drive: At least 50 GB of free space Online Connection Requirements: 512kbps minimum internet speed. Internet Connection required to install and play.","title":"Minimum Required Specifications"},{"location":"system_req/#recommended-specifications","text":"OS: Windows 10 \u2013 64-Bit CPU: AMD Athlon X4 870K, Intel i3 6300T or equivalent Memory: 8GB Graphics Card: AMD Radeon R9 270X, NVIDIA GeForce GTX 670, or equivalent Hard Drive: At least 50 GB of free space Online Connection Requirements: Broadband connection recommended. Internet Connection required to install and play.","title":"Recommended Specifications"}]}